{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this list as is\n",
    "number_cols = ['HP', 'Attack', 'Defense']\n",
    "\n",
    "# Remove the feature without variance from this list\n",
    "non_number_cols = ['Name', 'Type', 'Legendary']\n",
    "\n",
    "# Create a new dataframe by subselecting the chosen features\n",
    "df_selected = pokemon_df[number_cols + non_number_cols]\n",
    "\n",
    "# Prints the first 5 lines of the new dataframe\n",
    "print(df_selected.head())\n",
    "print(df_selected.describe(exclude='number'))\n",
    "non_number_cols.remove('Legendary')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Seaborn\n",
    "\n",
    "# Remove the redundant feature\n",
    "reduced_df = ansur_df_2.drop('n_legs', axis=1)\n",
    "\n",
    "# Create a pairplot and color the points using the 'Gender' feature\n",
    "sns.pairplot(reduced_df, hue='Gender', diag_kind='hist')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TSNE\n",
    "\n",
    "# Non-numerical columns in the dataset\n",
    "non_numeric = ['Branch', 'Gender', 'Component']\n",
    "\n",
    "# Drop the non-numerical columns from df\n",
    "df_numeric = df.drop(non_numeric, axis=1)\n",
    "\n",
    "# Create a t-SNE model with learning rate 50\n",
    "m = TSNE(learning_rate = 50)\n",
    "\n",
    "# Fit and transform the t-SNE model on the numeric dataset\n",
    "tsne_features = m.fit_transform(df_numeric)\n",
    "print(tsne_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Curse of Dimensionality\n",
    "\n",
    "# Assign just the 'neckcircumferencebase' column from ansur_df to X\n",
    "X = ansur_df[['neckcircumferencebase']]\n",
    "\n",
    "# Split the data, instantiate a classifier and fit the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "svc = SVC()\n",
    "svc.fit(X_train, y_train)\n",
    "\n",
    "# Calculate accuracy scores on both train and test data\n",
    "accuracy_train = accuracy_score(y_train, svc.predict(X_train))\n",
    "accuracy_test = accuracy_score(y_test, svc.predict(X_test))\n",
    "\n",
    "print(\"{0:.1%} accuracy on test set vs. {1:.1%} on training set\".format(accuracy_test, accuracy_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the boxplot\n",
    "head_df.boxplot()\n",
    "\n",
    "plt.show()\n",
    "# Normalize the data\n",
    "normalized_df = head_df / head_df.mean()\n",
    "\n",
    "normalized_df.boxplot()\n",
    "plt.show()\n",
    "\n",
    "# Normalize the data\n",
    "normalized_df = head_df / head_df.mean()\n",
    "\n",
    "# Print the variances of the normalized data\n",
    "print(normalized_df.var())\n",
    "\n",
    "# Normalize the data\n",
    "normalized_df = head_df / head_df.mean()\n",
    "\n",
    "# Print the variances of the normalized data\n",
    "print(normalized_df.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "# Create a VarianceThreshold feature selector\n",
    "sel = VarianceThreshold(threshold=0.001)\n",
    "\n",
    "# Fit the selector to normalized head_df\n",
    "sel.fit(head_df / head_df.mean())\n",
    "\n",
    "# Create a boolean mask\n",
    "mask = sel.get_support()\n",
    "\n",
    "# Apply the mask to create a reduced dataframe\n",
    "reduced_df = head_df.loc[:,mask]\n",
    "\n",
    "print(\"Dimensionality reduced from {} to {}.\".format(head_df.shape[1], reduced_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Graficando correlaciones\n",
    "\n",
    "# Create the correlation matrix\n",
    "corr = ansur_df.corr()\n",
    "\n",
    "# Draw the heatmap\n",
    "sns.heatmap(corr,  cmap=cmap, center=0, linewidths=1, annot=True, fmt=\".2f\")\n",
    "plt.show()\n",
    "# Create the correlation matrix\n",
    "corr = ansur_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Create the correlation matrix\n",
    "corr = ansur_df.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle \n",
    "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Add the mask to the heatmap\n",
    "sns.heatmap(corr, mask=mask, cmap=cmap, center=0, linewidths=1, annot=True, fmt=\".2f\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creando un MASK para filtrar resultados\n",
    "\n",
    "# Calculate the correlation matrix and take the absolute value\n",
    "corr_matrix = ansur_df.corr().abs()\n",
    "\n",
    "# Create a True/False mask and apply it\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "tri_df = corr_matrix.mask(mask)\n",
    "\n",
    "# List column names of highly correlated features (r > 0.95)\n",
    "to_drop = [c for c in tri_df.columns if any(tri_df[c] >  0.95)]\n",
    "\n",
    "# Drop the features in the to_drop list\n",
    "reduced_df = ansur_df.drop(to_drop, axis=1)\n",
    "\n",
    "print(\"The reduced dataframe has {} columns.\".format(reduced_df.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the first five lines of weird_df\n",
    "print(weird_df.head())\n",
    "# Put nuclear energy production on the x-axis and the number of pool drownings on the y-axis\n",
    "sns.scatterplot(x='nuclear_energy', y='pool_drownings', data=weird_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the scaler on the training features and transform these in one go\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# Fit the logistic regression model on the scaled training data\n",
    "lr.fit(X_train_std, y_train)\n",
    "\n",
    "# Scale the test features\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Predict diabetes presence on the scaled test set\n",
    "y_pred = lr.predict(X_test_std)\n",
    "\n",
    "# Prints accuracy metrics and feature coefficients\n",
    "print(\"{0:.1%} accuracy on test set.\".format(accuracy_score(y_test, y_pred))) \n",
    "print(dict(zip(X.columns, abs(lr.coef_[0]).round(2))))\n",
    "\n",
    "# Create the RFE with a LogisticRegression estimator and 3 features to select\n",
    "rfe = RFE(estimator=LogisticRegression(), n_features_to_select=3, verbose=1)\n",
    "\n",
    "# Fits the eliminator to the data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Print the features and their ranking (high = dropped early on)\n",
    "print(dict(zip(X.columns, rfe.ranking_)))\n",
    "\n",
    "# Print the features that are not eliminated\n",
    "print(X.columns[rfe.support_])\n",
    "\n",
    "# Calculates the test set accuracy\n",
    "acc = accuracy_score(y_test, rfe.predict(X_test))\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Trainning and test set\n",
    "# Perform a 75% training and 25% test data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\n",
    "\n",
    "# Fit the random forest model to the training data\n",
    "rf = RandomForestClassifier(random_state=0)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the accuracy\n",
    "acc = accuracy_score(y_test, rf.predict(X_train))\n",
    "\n",
    "# Print the importances per feature\n",
    "print(dict(zip(X.columns, rf.feature_importances_.round(2))))\n",
    "\n",
    "# Print accuracy\n",
    "print(\"{0:.1%} accuracy on test set.\".format(acc))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduciendo 2 features en cada paso\n",
    "# Set the feature eliminator to remove 2 features on each step\n",
    "rfe = RFE(estimator=RandomForestClassifier(), n_features_to_select=2, step=2, verbose=1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "# Create a mask\n",
    "mask = rfe.support_\n",
    "\n",
    "# Apply the mask to the feature dataset X and print the result\n",
    "reduced_X = X.loc[:, mask]\n",
    "print(reduced_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the test size to 30% to get a 70-30% train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Fit the scaler on the training features and transform these in one go\n",
    "X_train_std = scaler.fit_transform(X_train)\n",
    "\n",
    "# Create the Lasso model\n",
    "la = Lasso()\n",
    "\n",
    "# Fit it to the standardized training data\n",
    "la.fit(X_train_std, y_train)\n",
    "\n",
    "# Transform the test set with the pre-fitted scaler\n",
    "X_test_std = scaler.transform(X_test)\n",
    "\n",
    "# Calculate the coefficient of determination (R squared) on X_test_std\n",
    "r_squared = la.score(X_test_std, y_test)\n",
    "print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
    "\n",
    "# Create a list that has True values when coefficients equal 0\n",
    "zero_coef = la.coef_ == 0\n",
    "\n",
    "# Calculate how many features have a zero coefficient\n",
    "n_ignored = sum(zero_coef)\n",
    "print(\"The model has ignored {} out of {} features.\".format(n_ignored, len(la.coef_)))\n",
    "\n",
    "# Find the highest alpha value with R-squared above 98%\n",
    "la = Lasso(0.1, random_state=0)\n",
    "\n",
    "# Fits the model and calculates performance stats\n",
    "la.fit(X_train_std, y_train)\n",
    "r_squared = la.score(X_test_std, y_test)\n",
    "n_ignored_features = sum(la.coef_ == 0)\n",
    "\n",
    "# Print peformance stats \n",
    "print(\"The model can predict {0:.1%} of the variance in the test set.\".format(r_squared))\n",
    "print(\"{} out of {} features were ignored.\".format(n_ignored_features, len(la.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a MASK and a LASSOCV model to fit data\n",
    "\n",
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "# Create and fit the LassoCV model on the training set\n",
    "lcv = LassoCV()\n",
    "lcv.fit(X_train, y_train)\n",
    "print('Optimal alpha = {0:.3f}'.format(lcv.alpha_))\n",
    "\n",
    "# Calculate R squared on the test set\n",
    "r_squared = lcv.score(X_test, y_test)\n",
    "print('The model explains {0:.1%} of the test set variance'.format(r_squared))\n",
    "\n",
    "# Create a mask for coefficients not equal to zero\n",
    "lcv_mask = lcv.coef_ != 0\n",
    "print('{} features out of {} selected'.format(sum(lcv_mask), len(lcv_mask)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating a MASK and a LASSOCV model to fit data\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Select 10 features with RFE on a RandomForestRegressor, drop 3 features on each step\n",
    "rfe_rf = RFE(estimator=RandomForestRegressor(), \n",
    "             n_features_to_select=10, step=3, verbose=1)\n",
    "rfe_rf.fit(X_train, y_train)\n",
    "\n",
    "# Calculate the R squared on the test set\n",
    "r_squared = rfe_rf.score(X_test, y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set'.format(r_squared))\n",
    "\n",
    "# Assign the support array to gb_mask\n",
    "rf_mask = rfe_rf.support_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sum the votes of the three models\n",
    "votes = np.sum([lcv_mask, rf_mask, gb_mask], axis=0)\n",
    "\n",
    "# Create a mask for features selected by all 3 models\n",
    "meta_mask = votes >= 3\n",
    "\n",
    "# Apply the dimensionality reduction on X\n",
    "X_reduced = X.loc[:, meta_mask]\n",
    "\n",
    "# Plug the reduced dataset into a linear regression pipeline\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_reduced, y, test_size=0.3, random_state=0)\n",
    "lm.fit(scaler.fit_transform(X_train), y_train)\n",
    "r_squared = lm.score(scaler.transform(X_test), y_test)\n",
    "print('The model can explain {0:.1%} of the variance in the test set using {1:} features.'.format(r_squared, len(lm.coef_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Extraction of features\n",
    "\n",
    "# Calculate the price from the quantity sold and revenue\n",
    "sales_df['price'] = sales_df.revenue/sales_df.quantity\n",
    "\n",
    "# Drop the quantity and revenue features\n",
    "reduced_df = sales_df.drop(['revenue', 'quantity'], axis=1)\n",
    "\n",
    "print(reduced_df.head())\n",
    "\n",
    "# Calculate the mean height\n",
    "height_df['height'] = height_df[['height_1','height_2','height_3']].mean(axis=1)\n",
    "\n",
    "# Drop the 3 original height features\n",
    "reduced_df = height_df.drop(['height_1','height_2','height_3'], axis=1)\n",
    "\n",
    "print(reduced_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a pairplot to inspect ansur_df\n",
    "sns.pairplot(ansur_df)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Create the scaler\n",
    "scaler = StandardScaler()\n",
    "ansur_std = scaler.fit_transform(ansur_df)\n",
    "\n",
    "# Create the PCA instance and fit and transform the data with pca\n",
    "pca = PCA()\n",
    "pc = pca.fit_transform(ansur_std)\n",
    "pc_df = pd.DataFrame(pc, columns=['PC 1', 'PC 2', 'PC 3', 'PC 4'])\n",
    "\n",
    "# Create a pairplot of the principal component dataframe\n",
    "sns.pairplot(pc_df)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Scale the data\n",
    "scaler = StandardScaler()\n",
    "ansur_std = scaler.fit_transform(ansur_df)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "pca.fit(ansur_std)\n",
    "\n",
    "# Print the cumulative sum of the explained variance ratio\n",
    "print(pca.explained_variance_ratio_.cumsum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the pipeline\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "        \t\t ('reducer', PCA(n_components=2))])\n",
    "\n",
    "# Fit it to the dataset and extract the component vectors\n",
    "pipe.fit(poke_df)\n",
    "vectors = pipe.steps[1][1].components_.round(2)\n",
    "\n",
    "# Print feature effects\n",
    "print('PC 1 effects = ' + str(dict(zip(poke_df.columns, vectors[0]))))\n",
    "print('PC 2 effects = ' + str(dict(zip(poke_df.columns, vectors[1]))))\n",
    "\n",
    "pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                 ('reducer', PCA(n_components=2))])\n",
    "\n",
    "# Fit the pipeline to poke_df and transform the data\n",
    "pc = pipe.fit_transform(poke_df)\n",
    "\n",
    "# Add the 2 components to poke_cat_df\n",
    "poke_cat_df['PC 1'] = pc[:, 0]\n",
    "poke_cat_df['PC 2'] = pc[:, 1]\n",
    "\n",
    "# Use the Legendary feature to color the PC 1 vs PC 2 scatterplot\n",
    "sns.scatterplot(data=poke_cat_df, \n",
    "                x='PC 1', y='PC 2', hue='Legendary')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ajustando un modelo de predicci√≥n luego de determinar los PCA\n",
    "\n",
    "# Build the pipeline\n",
    "pipe = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('reducer', PCA(n_components=3)),\n",
    "        ('classifier', RandomForestClassifier(random_state=0))])\n",
    "\n",
    "# Fit the pipeline to the training data\n",
    "pipe.fit(X_train, y_train)\n",
    "\n",
    "# Score the accuracy on the test set\n",
    "accuracy = pipe.score(X_test, y_test)\n",
    "\n",
    "# Prints the explained variance ratio and accuracy\n",
    "print(pipe.steps[1][1].explained_variance_ratio_)\n",
    "print('{0:.1%} test set accuracy'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reverse transform\n",
    "# Transform the input data to principal components\n",
    "pc = pipe.transform(X_test)\n",
    "\n",
    "# Prints the number of features per dataset\n",
    "print(\"X_test has {} features\".format(X_test.shape[1]))\n",
    "print(\"pc has {} features\".format(pc.shape[1]))\n",
    "\n",
    "# Transform the input data to principal components\n",
    "pc = pipe.transform(X_test)\n",
    "\n",
    "# Inverse transform the components to original feature space\n",
    "X_rebuilt = pipe.inverse_transform(pc)\n",
    "\n",
    "# Prints the number of features\n",
    "print(\"X_rebuilt has {} features\".format(X_rebuilt.shape[1]))\n",
    "\n",
    "# Transform the input data to principal components\n",
    "pc = pipe.transform(X_test)\n",
    "\n",
    "# Inverse transform the components to original feature space\n",
    "X_rebuilt = pipe.inverse_transform(pc)\n",
    "\n",
    "# Plot the reconstructed data\n",
    "plot_digits(X_rebuilt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
